# TidalTransformer: 增强推理能力的双向块级注意力机制

## 注意力矩阵示例

考虑输入序列："Math Equation" 

在训练"Equation"的时候, 序列化成"Math"和逆序块: "Math <begin_of_block>noitauqE<end_of_block>"

`<begin_of_block>` = `<bob>`, `<end_of_block>` = `<eob>`

注意力矩阵如下（为简洁起见, 只显示部分元素）：

|       | Ma | th |    | <bob> | n | o | i | t | a | u | q | E | ... | <eob> |
|-------|----|----|----|-------|---|---|---|---|---|---|---|---|-----|-------|
| Ma    |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| th    |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
|       |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| <bob> |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| n     |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| o     |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| i     |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| t     |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| a     |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| u     |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| q     |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| E     |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| ...   |    |    |    |       |   |   |   |   |   |   |   |   |     |       |
| <eob> |    |    |    |       |   |   |   |   |   |   |   |   |     |       |

## 注意力机制说明

1. **块级处理**：
   - 输入被分为两个主要部分：普通文本 "Math" 和 逆序块 "<begin_of_block>noitauqE<end_of_block>"
   - 逆序块内的文本 "noitauqE" 是 "Equation" 的逆序表示

2. **处理顺序**：
   - 块外：正常tokenize, 从左到右处理（"Math" 到 "noitauqE"）
   - 块内：因为是逆序块, 从左到右处理字符（从 "n" 到 "E"）即为捕捉真实文本从后往前关系

3. **位置编码**：
   - 设计一种新的位置编码, 区分token和char的位置
   - 从 <bob>（begin of block）标记开始计算位置

4. **字符级生成和重新tokenization**：
   - 在块内, 逐个字符从左到右生成内容, 等同于从后往前生成文本, 显示时逆序即可
   - 生成完成后, 将字符序列重新tokenize为标准token, 继续生成下个逆序块

## 创新点

1. **双向信息流**：
   - 通过块间左右和块内逆序左右的处理, 实现了双向信息流
   - 有助于捕捉复杂的上下文关系, 特别是在数学表达式和逻辑推理中, 例如数学加法是从最后位开始进位

2. **细粒度处理**：
   - 字符级生成允许模型在更细的粒度上学习和生成内容
   - 对于数学符号和特殊字符的处理更为灵活

3. **动态上下文理解**：
   - 块级处理和双向流动使模型能够动态调整对上下文的理解
   - 有利于处理需要多步推理的复杂问题

4. **高效的长距离依赖建模**：
   - 实现char-level的生成, 更注重推理
   - 块生成结束后重新tokenize减少了注意力的计算复杂度
   - 新的位置编码有助于更好地建模长距离依赖

